# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: lrs2.yaml
  - override /model: resnet_transformer.yaml
  - override /callbacks: default.yaml
  - override /trainer: ddp.yaml
  - override /logger: tensorboard.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["lrs2", "resnet","transformer"]

seed: 4211

model:
  load_model: logs/resnet_transformer/train/runs/2023-02-13_12-50-46/checkpoints/epoch_056_val-wer_0.048194.ckpt

trainer:
  max_epochs: 200
  gradient_clip_val: 0.5
  check_val_every_n_epoch: 1
  accumulate_grad_batches: 10
  precision: 16

data:
  batch_size: 2
  num_workers: 4
  pretrain: False

callbacks:
  early_stopping:
    monitor: val/wer
    mode: min
    patience: 100

  model_checkpoint:
    filename: epoch_{epoch:03d}_val-wer_{val/wer:03f}
    monitor: val/wer
    mode: min

paths:
  log_dir: ${paths.root_dir}/logs/resnet_transformer

logger:
  tensorboard:
    name: resnet_transformer
    
